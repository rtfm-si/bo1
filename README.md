# Board of One (bo1)

AI-powered decision-making system that helps solve complex problems through structured decomposition, multi-perspective debate, and collaborative synthesis using AI personas.

## Project Status

**v1 Development Phase** - Week 1 foundation complete (Days 1-6), Day 7 integration testing in progress.

### Week 1 Progress (Days 1-7)
- âœ… Core Pydantic models (Problem, Persona, State, Votes)
- âœ… LLM client with prompt caching (90% cost reduction)
- âœ… Redis state management with serialization
- âœ… Console UI with Rich formatting
- âœ… 45 expert personas catalog
- âœ… Modular prompt composition system
- ðŸš§ Week 1 integration tests

## Quick Start

### Prerequisites

- Python 3.12+
- Docker & Docker Compose (recommended)
- API Keys:
  - [Anthropic API](https://console.anthropic.com/) (Claude)
  - [Voyage AI API](https://www.voyageai.com/) (Embeddings)

### Installation

#### Option 1: Docker (Recommended)

```bash
# 1. Clone and navigate to repository
cd bo1

# 2. Initial setup (creates .env, directories)
make setup

# 3. Edit .env and add your API keys
# Required: ANTHROPIC_API_KEY, VOYAGE_API_KEY

# 4. Build and start services
make build
make up

# 5. Run application
make run
```

#### Option 2: Local Development

```bash
# 1. Install uv package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. Create virtual environment and install dependencies
uv sync

# 3. Set up environment variables
cp .env.example .env
# Edit .env and add your API keys

# 4. Start Redis (required)
docker run -d -p 6379:6379 redis:7-alpine

# 5. Run application
uv run python -m bo1.main
```

## Development

### Running Tests

```bash
# Docker (recommended)
make test              # All tests
make test-unit         # Unit tests only

# Local environment
pytest                              # All tests
pytest -m unit                      # Unit tests only
pytest -m integration               # Integration tests only
pytest -m "not requires_llm"        # Skip tests requiring API keys

# Run Week 1 integration test
pytest tests/test_integration_day7.py -v

# Run with coverage
pytest --cov=bo1 --cov-report=html
```

### Code Quality

```bash
# Docker (recommended)
make lint              # Run linter (ruff check)
make format            # Run formatter (ruff format)
make check             # Run lint + typecheck

# Local environment
ruff check bo1/ tests/                           # Linting
ruff format bo1/ tests/                          # Formatting
mypy bo1/ --ignore-missing-imports               # Type checking

# Run all checks
ruff check bo1/ tests/ && ruff format --check bo1/ tests/ && mypy bo1/ --ignore-missing-imports
```

### Project Structure

```
bo1/
â”œâ”€â”€ bo1/                    # Main application package
â”‚   â”œâ”€â”€ agents/            # Agent implementations (Expert, Facilitator, etc.)
â”‚   â”œâ”€â”€ orchestration/     # Deliberation engine & session management
â”‚   â”œâ”€â”€ models/            # Pydantic models (Problem, Persona, State)
â”‚   â”œâ”€â”€ state/             # Redis state management
â”‚   â”œâ”€â”€ prompts/           # Prompt templates and framework
â”‚   â”œâ”€â”€ ui/                # Console UI (Rich)
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â””â”€â”€ main.py            # Application entry point
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ zzz_important/         # Documentation & design specs
â”‚   â”œâ”€â”€ PRD.md
â”‚   â”œâ”€â”€ IMPLEMENTATION_PROPOSAL.md
â”‚   â”œâ”€â”€ PROMPT_ENGINEERING_FRAMEWORK.md
â”‚   â”œâ”€â”€ CONSENSUS_BUILDING_RESEARCH.md
â”‚   â””â”€â”€ personas.json      # 35 pre-defined expert personas
â”œâ”€â”€ docker-compose.yml     # Docker services configuration
â”œâ”€â”€ Dockerfile             # Application container
â””â”€â”€ pyproject.toml         # Python package configuration
```

## Documentation

- **[CLAUDE.md](./CLAUDE.md)** - Developer guide for working with this codebase
- **[PRD.md](./zzz_important/PRD.md)** - Product requirements & user stories
- **[Implementation Proposal](./zzz_important/IMPLEMENTATION_PROPOSAL.md)** - Technical architecture
- **[Prompt Engineering Framework](./zzz_important/PROMPT_ENGINEERING_FRAMEWORK.md)** - AI prompt guidelines
- **[Consensus Building Research](./zzz_important/CONSENSUS_BUILDING_RESEARCH.md)** - Research-backed techniques

## Architecture

### System Flow

```
Problem Intake â†’ Decomposition (1-5 sub-problems) â†’ Expert Selection (3-5 personas)
â†’ Multi-Round Debate (adaptive rounds) â†’ Voting â†’ Synthesis â†’ Final Recommendation
```

### Prompt Engineering Framework

Board of One uses a **modular composition** approach for AI prompts:

#### 3-Layer Composition
```
Final Prompt = BESPOKE IDENTITY + GENERIC PROTOCOLS + DYNAMIC CONTEXT
```

1. **Bespoke Identity** (from `personas.json`)
   - Unique system role for each of 45 experts
   - ~879 characters average per persona
   - Examples: "You are Maria Chen, a growth hacker..."

2. **Generic Protocols** (from `reusable_prompts.py`)
   - `BEHAVIORAL_GUIDELINES`: Communication norms
   - `EVIDENCE_PROTOCOL`: Reasoning standards
   - `COMMUNICATION_PROTOCOL`: XML output format
   - `SECURITY_PROTOCOL`: Safety guardrails
   - **Cached for 90% cost reduction**

3. **Dynamic Context** (per request)
   - Problem statement
   - Participant list
   - Current phase (initial_round, discussion, voting)
   - Previous contributions (hierarchical summaries)

#### Why This Matters
- **DRY**: Generic protocols reused across all personas
- **Caching**: Protocols + problem statement cached = massive savings
- **Maintainability**: Update protocols once, affects all personas
- **Consistency**: All personas follow same behavioral norms

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Language | Python 3.12+ | Core implementation |
| Package Manager | uv | Fast dependency management |
| LLM | Claude Haiku 4.5 + Sonnet 4.5 | AI reasoning |
| Orchestration | LangChain | LLM workflow management |
| State | Redis | In-memory session state |
| Embeddings | Voyage AI | Semantic similarity |
| Console UI | Rich | Beautiful terminal output |

## Cost Optimization

Board of One achieves ~$0.10 per deliberation through:

### Prompt Caching (90% savings)
- Generic protocols (BEHAVIORAL_GUIDELINES, EVIDENCE_PROTOCOL) are cached
- Problem statements cached across all persona calls
- Round summaries cached across subsequent rounds
- **Sonnet with caching** is cheaper than Haiku without!

### Model Allocation
| Role | Model | Rationale |
|------|-------|-----------|
| Personas | Sonnet 4.5 + cache | Complex reasoning, high reuse |
| Facilitator | Sonnet 4.5 | Orchestration requires reasoning |
| Summarizer | Haiku 4.5 | Simple compression task |
| Decomposer | Sonnet 4.5 | Complex problem analysis |
| Moderators | Haiku 4.5 | Simple interventions |

### Hierarchical Context
- **Old rounds**: 100-token summaries (cached)
- **Current round**: Full messages (uncached)
- **Total context**: ~1,400 tokens (linear growth, not quadratic)
- **Async summarization**: Zero latency impact (runs in background)

### Target Costs (Per Deliberation)
- 35 persona contributions (Sonnet + cache): ~$0.095
- 6 round summaries (Haiku): ~$0.007
- Facilitator decisions (Sonnet): ~$0.003
- **Total**: ~$0.10 per deliberation (70% cheaper than naive implementation)

## Contributing

This is currently a development project. See [CLAUDE.md](./CLAUDE.md) for development guidelines.

## License

TBD
