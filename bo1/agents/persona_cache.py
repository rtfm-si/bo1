"""Persona selection caching with semantic similarity.

This module provides intelligent caching of persona selections to reduce LLM API costs
by 40-60% through semantic similarity matching of problem goals.

Features:
- Semantic similarity-based matching using Voyage AI embeddings (voyage-3, 1024 dimensions)
- Redis-backed storage with automatic expiration
- Cosine similarity threshold: 0.90 (higher than research cache for accuracy)
- Cache hit/miss tracking and statistics
- Configurable TTL (default: 7 days)
- Graceful degradation if caching fails

Cost Impact:
- Cache hits: ~$0.00006 (embedding only)
- Cache misses: ~$0.01-0.02 (LLM persona selection call)
- Expected hit rate: 40-60% for similar problems
- Cost savings: ~$200-400/month at 1000 deliberations
"""

import json
import logging
from typing import Any

from bo1.config import get_settings
from bo1.llm.embeddings import cosine_similarity, generate_embedding
from bo1.models.persona import PersonaProfile
from bo1.models.problem import SubProblem

logger = logging.getLogger(__name__)


class PersonaSelectionCache:
    """Semantic similarity-based persona selection cache with Redis backend.

    This cache stores persona selections using embeddings of problem goals.
    When a new problem arrives, we search for similar cached problems using
    cosine similarity and return cached personas if similarity exceeds threshold.

    Examples:
        >>> from bo1.state.redis_manager import get_redis_manager
        >>> cache = PersonaSelectionCache(get_redis_manager())
        >>> problem = SubProblem(goal="Should we expand to EU?", ...)
        >>> cached = await cache.get_cached_personas(problem)
        >>> if cached:
        ...     print(f"Cache hit! {len(cached)} personas")
        >>> else:
        ...     # Perform LLM selection
        ...     personas = await selector.recommend_personas(problem)
        ...     await cache.cache_persona_selection(problem, personas)
    """

    def __init__(self, redis_manager: Any) -> None:
        """Initialize persona selection cache.

        Args:
            redis_manager: RedisManager instance for cache storage
        """
        self.redis = redis_manager.redis
        self.enabled = get_settings().enable_persona_selection_cache
        self.similarity_threshold = 0.90  # Higher than research cache (0.85) for accuracy
        self.ttl_seconds = 7 * 24 * 60 * 60  # 7 days (longer than research cache)
        self._hits = 0
        self._misses = 0

    async def get_cached_personas(
        self,
        problem: SubProblem,
    ) -> list[PersonaProfile] | None:
        """Get cached persona selection if similar problem exists.

        Args:
            problem: SubProblem to get personas for

        Returns:
            Cached persona list if similar problem found, else None

        Note:
            This method performs O(N) similarity search across all cached entries.
            For production at scale, consider pgvector or Pinecone integration.
        """
        if not self.enabled:
            logger.debug("Persona selection cache disabled")
            return None

        try:
            # Generate embedding for problem goal
            query_embedding = generate_embedding(problem.goal, input_type="query")

            # Search cache for similar problems
            # Note: This iterates all keys - acceptable for moderate cache sizes (<1000 entries)
            # For larger scale, migrate to pgvector like research cache
            cache_keys = self.redis.keys("personas:cache:*")

            best_match_personas = None
            best_similarity = self.similarity_threshold

            for key in cache_keys:
                cached_data_json = self.redis.get(key)
                if not cached_data_json:
                    continue

                cached_data = json.loads(cached_data_json)
                cached_embedding = cached_data.get("embedding")

                if not cached_embedding:
                    logger.warning(f"Cache entry {key} missing embedding, skipping")
                    continue

                # Calculate similarity
                similarity = cosine_similarity(query_embedding, cached_embedding)

                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match_personas = cached_data.get("personas")

            if best_match_personas:
                self._hits += 1
                logger.info(
                    f"âœ“ Persona cache hit (similarity={best_similarity:.3f}, "
                    f"threshold={self.similarity_threshold})"
                )

                # Convert persona dicts to PersonaProfile models
                return [PersonaProfile(**p) for p in best_match_personas]

            # No similar problem found
            self._misses += 1
            logger.debug(
                f"Persona cache miss (best similarity: {best_similarity:.3f}, "
                f"threshold: {self.similarity_threshold})"
            )
            return None

        except Exception as e:
            logger.error(f"Persona cache read error: {e}", exc_info=True)
            self._misses += 1
            return None

    async def cache_persona_selection(
        self,
        problem: SubProblem,
        personas: list[PersonaProfile],
    ) -> None:
        """Cache persona selection for problem.

        Args:
            problem: SubProblem personas were selected for
            personas: Selected personas to cache
        """
        if not self.enabled:
            return

        try:
            # Generate embedding
            embedding = generate_embedding(problem.goal, input_type="document")

            # Create cache key (hash of problem goal for uniqueness)
            import hashlib

            problem_hash = hashlib.sha256(problem.goal.encode()).hexdigest()[:16]
            cache_key = f"personas:cache:{problem_hash}"

            # Store in cache
            cache_data = {
                "embedding": embedding,
                "personas": [p.model_dump() for p in personas],
                "problem_goal": problem.goal,  # For debugging
                "problem_complexity": problem.complexity_score,
                "cached_at": None,  # Redis doesn't need this, TTL handles it
            }

            self.redis.setex(
                cache_key,
                self.ttl_seconds,
                json.dumps(cache_data),
            )

            logger.debug(
                f"Cached persona selection: {cache_key} "
                f"({len(personas)} personas, TTL: {self.ttl_seconds}s)"
            )

        except Exception as e:
            logger.error(f"Persona cache write error: {e}", exc_info=True)

    @property
    def hit_rate(self) -> float:
        """Calculate cache hit rate.

        Returns:
            Hit rate as decimal (0.0-1.0), or 0.0 if no requests yet
        """
        total = self._hits + self._misses
        return self._hits / total if total > 0 else 0.0

    def get_stats(self) -> dict[str, Any]:
        """Get cache statistics.

        Returns:
            Dictionary with cache performance metrics
        """
        return {
            "enabled": self.enabled,
            "hits": self._hits,
            "misses": self._misses,
            "hit_rate": self.hit_rate,
            "similarity_threshold": self.similarity_threshold,
            "ttl_seconds": self.ttl_seconds,
            "ttl_days": self.ttl_seconds / (24 * 60 * 60),
        }


# Global cache instance
_persona_cache: PersonaSelectionCache | None = None


def get_persona_cache() -> PersonaSelectionCache:
    """Get or create persona cache instance.

    Returns:
        PersonaSelectionCache singleton instance

    Examples:
        >>> cache = get_persona_cache()
        >>> print(cache.get_stats())
    """
    global _persona_cache
    if _persona_cache is None:
        from bo1.state.redis_manager import RedisManager

        _persona_cache = PersonaSelectionCache(RedisManager())
    return _persona_cache
