# Prometheus alerting rules for Board of One
groups:
  - name: graph_node_alerts
    rules:
      # Alert when any graph node p95 latency exceeds 30 seconds
      - alert: GraphNodeSlowExecution
        expr: histogram_quantile(0.95, sum(rate(bo1_graph_node_duration_seconds_bucket[5m])) by (le, node_name)) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Graph node {{ $labels.node_name }} has high latency"
          description: "Node {{ $labels.node_name }} p95 latency is {{ $value | printf \"%.1f\" }}s (threshold: 30s)"

      # Alert when any graph node error rate exceeds 5%
      - alert: GraphNodeHighErrorRate
        expr: |
          sum(rate(bo1_graph_node_total{status="error"}[5m])) by (node_name)
          /
          sum(rate(bo1_graph_node_total[5m])) by (node_name)
          > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Graph node {{ $labels.node_name }} has high error rate"
          description: "Node {{ $labels.node_name }} error rate is {{ $value | printf \"%.1f%%\" }} (threshold: 5%)"

      # Critical alert when node p95 exceeds 60 seconds
      - alert: GraphNodeCriticalLatency
        expr: histogram_quantile(0.95, sum(rate(bo1_graph_node_duration_seconds_bucket[5m])) by (le, node_name)) > 60
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Graph node {{ $labels.node_name }} latency exceeds 60s"
          description: "Node {{ $labels.node_name }} p95 latency is {{ $value | printf \"%.1f\" }}s - investigate immediately"

      # Critical alert when error rate exceeds 20%
      - alert: GraphNodeCriticalErrorRate
        expr: |
          sum(rate(bo1_graph_node_total{status="error"}[5m])) by (node_name)
          /
          sum(rate(bo1_graph_node_total[5m])) by (node_name)
          > 0.20
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Graph node {{ $labels.node_name }} error rate exceeds 20%"
          description: "Node {{ $labels.node_name }} error rate is {{ $value | printf \"%.1f%%\" }} - investigate immediately"

  - name: db_pool_alerts
    rules:
      # Warning when DB pool utilization exceeds 80% for 5 minutes
      - alert: DBPoolHighUtilization
        expr: db_pool_utilization_percent > 0.80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool utilization is high"
          description: "DB pool utilization is {{ $value | printf \"%.1f%%\" }} (threshold: 80%)"

      # Critical when DB pool utilization exceeds 95% for 2 minutes
      - alert: DBPoolCritical
        expr: db_pool_utilization_percent > 0.95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Database connection pool near exhaustion"
          description: "DB pool utilization is {{ $value | printf \"%.1f%%\" }} (threshold: 95%) - investigate immediately"

      # Alert when pool degradation mode is active
      - alert: DBPoolDegraded
        expr: bo1_db_pool_degraded == 1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool in degraded mode"
          description: "DB pool has entered degraded mode due to exhaustion - falling back to reduced capacity"

      # Alert when connection queue depth is high
      - alert: DBPoolQueueBacklog
        expr: bo1_db_pool_queue_depth > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool queue backlog"
          description: "DB pool queue depth is {{ $value }} connections waiting (threshold: 10)"

  - name: circuit_breaker_alerts
    rules:
      # Critical: Circuit breaker is OPEN for any LLM provider (Anthropic, OpenAI, Voyage)
      - alert: CircuitBreakerOpen
        expr: circuit_breaker_state{state="open"} == 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Circuit breaker OPEN for {{ $labels.provider }}"
          description: "Circuit breaker for {{ $labels.provider }} has been OPEN for >1 minute. Service is experiencing repeated failures and requests are being fast-failed."

      # Warning: Circuit breaker is HALF_OPEN for extended period (recovery taking too long)
      - alert: CircuitBreakerHalfOpen
        expr: circuit_breaker_state{state="half_open"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Circuit breaker HALF_OPEN for {{ $labels.provider }} (extended recovery)"
          description: "Circuit breaker for {{ $labels.provider }} has been in HALF_OPEN state for >5 minutes. Service recovery is taking longer than expected."

      # Warning: High rate of circuit breaker trips
      - alert: CircuitBreakerTripsHigh
        expr: increase(bo1_circuit_breaker_trips_total[15m]) > 3
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Multiple circuit breaker trips for {{ $labels.service }}"
          description: "Circuit breaker for {{ $labels.service }} has tripped {{ $value | printf \"%.0f\" }} times in the last 15 minutes. Provider may be unstable."

      # Critical: All LLM providers down
      - alert: AllLLMProvidersDown
        expr: |
          (circuit_breaker_state{provider="anthropic", state="open"} == 1)
          and
          (circuit_breaker_state{provider="openai", state="open"} == 1)
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "All LLM providers (Anthropic + OpenAI) are down"
          description: "Both Anthropic and OpenAI circuit breakers are OPEN. No fallback available. User requests will fail."

  - name: event_persistence_alerts
    rules:
      # Warning: Event persistence retry queue backing up
      - alert: EventPersistenceBacklog
        expr: bo1_event_persistence_retry_queue_depth > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Event persistence retry queue backlog"
          description: "Event persistence retry queue depth is {{ $value }} (threshold: 50). Events may be delayed."

      # Warning: High rate of event persistence retries
      - alert: EventPersistenceHighRetryRate
        expr: rate(bo1_event_persistence_retries_total[5m]) * 60 > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High event persistence retry rate"
          description: "Event persistence retry rate is {{ $value | printf \"%.1f\" }}/min (threshold: 10/min). Database may be under stress."

      # Warning: Event persistence flush latency is high
      - alert: EventPersistenceSlowFlush
        expr: histogram_quantile(0.95, sum(rate(bo1_event_persistence_duration_seconds_bucket[5m])) by (le)) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Event persistence flush latency is high"
          description: "Event persistence p95 flush duration is {{ $value | printf \"%.2f\" }}s (threshold: 2s)."

  - name: cost_tracking_alerts
    rules:
      # Warning: Cost tracking retry queue backing up
      - alert: CostTrackingBacklog
        expr: bo1_cost_retry_queue_depth > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Cost tracking retry queue backlog"
          description: "Cost tracking retry queue depth is {{ $value }} (threshold: 100). Cost records may be delayed."

      # Warning: High cost tracking failure rate
      - alert: CostTrackingHighFailureRate
        expr: |
          rate(bo1_cost_flush_total{status="failure"}[5m])
          /
          (rate(bo1_cost_flush_total{status="success"}[5m]) + rate(bo1_cost_flush_total{status="failure"}[5m]))
          > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High cost tracking failure rate"
          description: "Cost tracking failure rate is {{ $value | printf \"%.1f%%\" }} (threshold: 5%)."

      # Warning: Cost anomalies detected
      - alert: CostAnomalyDetected
        expr: increase(bo1_cost_anomaly_total[15m]) > 3
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Cost anomalies detected"
          description: "{{ $value | printf \"%.0f\" }} cost anomalies detected in the last 15 minutes (threshold: 3). Check for unusual LLM usage patterns."

      # Warning: Cost tracking flush latency is high
      - alert: CostTrackingSlowFlush
        expr: histogram_quantile(0.95, sum(rate(bo1_cost_flush_duration_seconds_bucket[5m])) by (le)) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Cost tracking flush latency is high"
          description: "Cost tracking p95 flush duration is {{ $value | printf \"%.2f\" }}s (threshold: 1s)."
