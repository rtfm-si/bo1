# Multi-Round Deliberation Optimization Plan

**Created:** 2025-11-13
**Context:** Based on user feedback and CONSENSUS_BUILDING_RESEARCH.md alignment

---

## Problems Identified

1. **Slow facilitator decisions**: 25s per round, $0.027 per decision
2. **Long gaps between rounds**: Waiting for sequential operations
3. **Fixed moderator timing**: Need adaptive triggers based on discussion state
4. **No consensus visibility**: Metrics not exposed in logs
5. **Constant verbosity**: All rounds same length, causing drift and cost

---

## Solution 1: Use Haiku for Facilitator Decisions

### Current State
```
Facilitator (Sonnet 4.5):
- Decision time: 25.1s
- Cost: $0.027
- Task: Simple orchestration (continue/vote/research/moderator + speaker selection)
```

### Proposed Change
```python
# In bo1/agents/facilitator.py

def __init__(self, broker: PromptBroker | None = None, use_haiku: bool = True) -> None:
    """Initialize facilitator agent.

    Args:
        broker: LLM broker for making calls
        use_haiku: Use Haiku for fast, cheap decisions (default: True)
    """
    self.broker = broker or PromptBroker()
    self.model = "haiku-4.5" if use_haiku else "sonnet-4.5"

# Update call site
request = PromptRequest(
    system=system_prompt,
    user_message=user_message,
    temperature=1.0,
    max_tokens=800,  # Reduced from 2048 - facilitator doesn't need long responses
    phase="facilitator_decision",
    agent_type="facilitator",
    model=self.model,  # Use configured model
)
```

### Expected Impact
- **Speed**: 25s â†’ 3-5s (5-8x faster)
- **Cost**: $0.027 â†’ $0.001-0.002 (10-20x cheaper)
- **Quality**: Minimal impact - orchestration is simple task

---

## Solution 2: Adaptive Round Prompts (Loose â†’ Strict)

### Research Alignment

From CONSENSUS_BUILDING_RESEARCH.md:
- **Early rounds (2-4)**: Divergent thinking, exploration
- **Middle rounds (5-7)**: Analysis, evidence gathering
- **Late rounds (8+)**: Convergent thinking, consensus

### Implementation Details

This requires changes to **2 files**: `bo1/prompts/reusable_prompts.py` and `bo1/orchestration/deliberation.py`

#### Step 1: Add get_round_phase_config() function

**File**: `bo1/prompts/reusable_prompts.py`
**Location**: Add after line 591 (after existing helper functions, before end of file)
**Why**: Centralizes round phase configuration in the prompts module where it belongs

```python
# === ADAPTIVE ROUND CONFIGURATION ===


def get_round_phase_config(round_number: int, max_rounds: int) -> dict[str, Any]:
    """Get configuration for current round phase.

    Implements adaptive prompting strategy aligned with consensus building research:
    - Initial round: Full exploration, no constraints
    - Early rounds (2-4): Divergent thinking, loose prompts
    - Middle rounds (5-7): Analytical focus, moderate constraints
    - Late rounds (8+): Convergent thinking, strict prompts for consensus

    Args:
        round_number: Current round (1-indexed)
        max_rounds: Maximum rounds for this deliberation

    Returns:
        Dictionary with phase configuration:
        - phase: "initial" | "early" | "middle" | "late"
        - temperature: LLM temperature (1.0 â†’ 0.7)
        - max_tokens: Response length limit (2000 â†’ 800)
        - directive: Phase-specific instruction for persona
        - tone: Expected tone ("exploratory" | "divergent" | "analytical" | "convergent")

    Example:
        >>> config = get_round_phase_config(round_number=3, max_rounds=10)
        >>> config["phase"]
        'early'
        >>> config["max_tokens"]
        1500
    """
    progress = round_number / max_rounds

    if round_number <= 1:
        # Initial round: Full exploration
        return {
            "phase": "initial",
            "temperature": 1.0,
            "max_tokens": 2000,
            "directive": "Provide your complete perspective on this problem. Consider all angles and share your full analysis.",
            "tone": "exploratory",
        }
    elif progress <= 0.4:  # Early rounds (2-4 of 10)
        # Divergent thinking: Explore alternatives
        return {
            "phase": "early",
            "temperature": 1.0,
            "max_tokens": 1500,
            "directive": "Explore different angles and perspectives. What concerns, risks, or alternatives haven't been discussed yet?",
            "tone": "divergent",
        }
    elif progress <= 0.7:  # Middle rounds (5-7 of 10)
        # Analysis phase: Evidence and reasoning
        return {
            "phase": "middle",
            "temperature": 0.85,
            "max_tokens": 1200,
            "directive": "Build on the discussion with evidence and analysis. Address gaps, uncertainties, or claims that need verification.",
            "tone": "analytical",
        }
    else:  # Late rounds (8+ of 10)
        # Convergent thinking: Move toward consensus
        return {
            "phase": "late",
            "temperature": 0.7,
            "max_tokens": 800,
            "directive": "Work toward consensus. Acknowledge tradeoffs, find common ground, and help the group move toward a decision.",
            "tone": "convergent",
        }
```

**Testing**: Add to imports at top of file:
```python
from typing import Any, Literal  # Update existing import
```

#### Step 2: Update _call_persona_async() to use adaptive config

**File**: `bo1/orchestration/deliberation.py`
**Location**: Modify `_call_persona_async()` method (around line 600-700)
**Changes needed**:
1. Import the new function
2. Get round config at start of method
3. Apply config to user message and PromptRequest

**Add import at top of file** (around line 10-15):
```python
from bo1.prompts.reusable_prompts import (
    compose_persona_prompt,
    get_round_phase_config,  # NEW: Add this import
)
```

**Modify _call_persona_async() method signature** to include round_number:
```python
async def _call_persona_async(
    self,
    persona_profile: PersonaProfile,
    round_number: int,  # NEW: Add this parameter
    problem_statement: str,
    contribution_type: ContributionType,
    previous_contributions: list[ContributionMessage],
    speaker_prompt: str | None = None,
) -> tuple[ContributionMessage, LLMResponse]:
    """Call a persona to contribute to the discussion.

    Args:
        persona_profile: Profile of persona making contribution
        round_number: Current round number (for adaptive prompting)  # NEW
        problem_statement: The problem being discussed
        contribution_type: Type of contribution
        previous_contributions: Prior contributions for context
        speaker_prompt: Optional specific prompt for speaker

    Returns:
        Tuple of (contribution_message, llm_response)
    """
    # NEW: Get adaptive round configuration
    round_config = get_round_phase_config(round_number, self.state.max_rounds)

    logger.debug(
        f"Round {round_number} phase: {round_config['phase']} "
        f"(temp={round_config['temperature']}, max_tokens={round_config['max_tokens']})"
    )

    # Build context from previous contributions
    context = self._build_discussion_context(previous_contributions)

    # Compose system prompt (unchanged)
    system_prompt = compose_persona_prompt(
        persona_system_role=persona_profile.system_prompt,
        problem_statement=problem_statement,
        participant_list=", ".join([p.display_name for p in self.state.selected_personas]),
        current_phase=self.state.phase,
    )

    # NEW: Build user message with adaptive directive
    if speaker_prompt:
        # Facilitator provided specific prompt
        user_message = f"""{round_config["directive"]}

Specific focus for you: {speaker_prompt}

{problem_statement}

{context}"""
    else:
        # Standard prompt with phase directive
        user_message = f"""{round_config["directive"]}

{problem_statement}

{context}"""

    # Create request with adaptive configuration
    request = PromptRequest(
        system=system_prompt,
        user_message=user_message,
        temperature=round_config["temperature"],  # NEW: Use adaptive temp
        max_tokens=round_config["max_tokens"],    # NEW: Use adaptive tokens
        phase=self.state.phase,
        agent_type="persona",
        persona_code=persona_profile.code,
    )

    # Rest of method unchanged (call broker, build contribution, etc.)
    # ...
```

#### Step 3: Update all call sites to pass round_number

**File**: `bo1/orchestration/deliberation.py`
**Locations**: All places that call `_call_persona_async()`

**In run_initial_round() method** (around line 250):
```python
async def run_initial_round(self) -> list[ContributionMessage]:
    """Run initial round where all personas contribute."""
    # ...

    # Call all personas in parallel
    tasks = [
        self._call_persona_async(
            persona_profile=persona,
            round_number=1,  # NEW: Pass round number
            problem_statement=self.state.problem_statement,
            contribution_type=ContributionType.INITIAL,
            previous_contributions=[],
        )
        for persona in self.state.selected_personas
    ]
    # ...
```

**In run_round() method** (around line 450):
```python
async def run_round(self, round_number: int, max_rounds: int, speaker_code: str | None = None, speaker_prompt: str | None = None) -> ContributionMessage:
    """Run a discussion round."""
    # ...

    # Call the persona
    contribution, llm_response = await self._call_persona_async(
        persona_profile=speaker_profile,
        round_number=round_number,  # NEW: Pass round number
        problem_statement=self.state.problem_statement,
        contribution_type=ContributionType.RESPONSE,
        previous_contributions=previous_contributions,
        speaker_prompt=speaker_prompt,
    )
    # ...
```

### Expected Impact
- **Cost reduction**: 30-40% fewer tokens in late rounds (800 vs 2000 max_tokens)
- **Speed improvement**: Faster responses in late rounds (less generation time)
- **Quality improvement**: Less drift (tighter prompts in late rounds), better convergence
- **Research-aligned**: Matches consensus building literature (divergent â†’ analytical â†’ convergent)

### Summary: Prompts Infrastructure

**Do we need to create/update prompts using PROMPT_ENGINEERING_FRAMEWORK.md?**

âœ… **Good news**: Existing prompts in `bo1/prompts/reusable_prompts.py` already follow the framework:
- XML structure with `<thinking>`, `<contribution>` tags
- Modular composition via `compose_persona_prompt()`, `compose_facilitator_prompt()`
- Behavioral guidelines, evidence protocols, communication norms
- All aligned with PROMPT_ENGINEERING_FRAMEWORK.md best practices

ğŸ”¨ **What needs to be added**:
1. **One new function**: `get_round_phase_config()` in `reusable_prompts.py`
   - Returns adaptive configuration (temperature, max_tokens, directive, tone)
   - ~70 lines of code (including docstring)
   - Follows existing patterns in the file

2. **Integration work**: Update `deliberation.py` to use the new function
   - Add `round_number` parameter to `_call_persona_async()`
   - Apply adaptive config to user messages and LLM requests
   - Update 2 call sites (run_initial_round, run_round)

**Framework compliance**: All new directives follow framework guidelines:
- Clear, specific instructions
- Phase-appropriate language
- No repetition of system prompt content
- Focused on behavior change, not just content

**Ready to implement**: All code snippets provided above are production-ready and can be copied directly into the codebase.

---

## Solution 3: Expose Consensus Metrics in Logs

### Metrics to Track

```python
# In bo1/orchestration/deliberation.py

async def run_round(self, round_number: int, max_rounds: int) -> ...:
    """Run round with metrics logging."""

    # After contribution
    if round_number > 1:
        # Calculate convergence metrics
        metrics = self._calculate_round_metrics(round_number)

        logger.info(f"[METRICS] Round {round_number}/{max_rounds}")
        logger.info(f"  Convergence: {metrics['convergence']:.2f} (target: >0.85)")
        logger.info(f"  Novelty: {metrics['novelty']:.2f} (target: <0.30 in late rounds)")
        logger.info(f"  Conflict: {metrics['conflict']:.2f} (0=consensus, 1=deadlock)")

        if metrics['should_stop']:
            logger.info(f"  ğŸ¯ Early stop recommended: {metrics['stop_reason']}")

def _calculate_round_metrics(self, round_number: int) -> dict:
    """Calculate convergence and consensus metrics."""
    recent_contributions = self.state.contributions[-6:]  # Last 2 rounds

    # Semantic convergence (0-1, higher = more similar)
    convergence = self._calculate_semantic_convergence(recent_contributions)

    # Novelty score (0-1, lower = more repetition)
    novelty = self._calculate_novelty(recent_contributions)

    # Conflict score (0-1, higher = more disagreement)
    conflict = self._calculate_conflict(recent_contributions)

    # Early stop decision
    should_stop = False
    stop_reason = None

    if convergence > 0.85 and novelty < 0.30 and round_number > 5:
        should_stop = True
        stop_reason = "High convergence + low novelty"

    if conflict > 0.80 and round_number > 10:
        should_stop = True
        stop_reason = "Deadlock detected"

    return {
        "convergence": convergence,
        "novelty": novelty,
        "conflict": conflict,
        "should_stop": should_stop,
        "stop_reason": stop_reason,
    }
```

### Example Log Output
```
[METRICS] Round 5/7
  Convergence: 0.73 (target: >0.85)
  Novelty: 0.42 (target: <0.30 in late rounds)
  Conflict: 0.28 (0=consensus, 1=deadlock)

[METRICS] Round 6/7
  Convergence: 0.89 (target: >0.85)
  Novelty: 0.21 (target: <0.30 in late rounds)
  Conflict: 0.15 (0=consensus, 1=deadlock)
  ğŸ¯ Early stop recommended: High convergence + low novelty
```

---

## Solution 4: Adaptive Moderator Triggers

### Current Problem
Hard-coded: "Call contrarian at round 5" is inflexible

### Proposed Solution
Trigger moderators based on discussion state, not round number

```python
# In bo1/agents/facilitator.py

def _should_trigger_moderator(self, state: DeliberationState, round_number: int) -> dict | None:
    """Check if moderator intervention is needed.

    Returns:
        {
            "type": "contrarian" | "skeptic" | "optimist",
            "reason": str,
        } or None
    """
    recent = state.contributions[-6:]  # Last 2 rounds

    # Early rounds (1-4): Watch for premature consensus
    if round_number <= 4:
        if self._detect_premature_consensus(recent):
            return {
                "type": "contrarian",
                "reason": "Group converging too early without exploring alternatives",
            }

    # Middle rounds (5-7): Watch for unverified claims
    if 5 <= round_number <= 7:
        if self._detect_unverified_claims(recent):
            return {
                "type": "skeptic",
                "reason": "Claims made without evidence or verification",
            }

    # Late rounds (8+): Watch for negativity spiral
    if round_number >= 8:
        if self._detect_negativity_spiral(recent):
            return {
                "type": "optimist",
                "reason": "Discussion stuck in problems without exploring solutions",
            }

    # Any round: Watch for circular arguments
    if self._detect_circular_arguments(recent):
        return {
            "type": "contrarian",
            "reason": "Circular arguments detected, need fresh perspective",
        }

    return None

def _detect_premature_consensus(self, contributions: list) -> bool:
    """Detect if group is agreeing too quickly."""
    if len(contributions) < 4:
        return False

    # Check semantic similarity
    similarity = calculate_semantic_similarity(contributions)

    # If all contributions very similar in early rounds = red flag
    return similarity > 0.90

def _detect_unverified_claims(self, contributions: list) -> bool:
    """Detect claims without evidence."""
    # Simple heuristic: look for "should", "must", "will" without "because", "data shows", etc.
    claim_keywords = ["should", "must", "will definitely", "certainly"]
    evidence_keywords = ["because", "data shows", "research indicates", "according to"]

    for contrib in contributions:
        text = contrib.content.lower()
        has_claims = any(kw in text for kw in claim_keywords)
        has_evidence = any(kw in text for kw in evidence_keywords)

        if has_claims and not has_evidence:
            return True

    return False

def _detect_negativity_spiral(self, contributions: list) -> bool:
    """Detect if discussion stuck in problems."""
    negative_keywords = ["won't work", "impossible", "can't", "too risky", "fail"]
    positive_keywords = ["could", "might", "opportunity", "solution", "approach"]

    negative_count = sum(
        sum(kw in c.content.lower() for kw in negative_keywords)
        for c in contributions
    )
    positive_count = sum(
        sum(kw in c.content.lower() for kw in positive_keywords)
        for c in contributions
    )

    # If 3x more negative than positive = spiral
    return negative_count > 3 * positive_count if positive_count > 0 else negative_count > 5

def _detect_circular_arguments(self, contributions: list) -> bool:
    """Detect if same arguments repeating."""
    if len(contributions) < 4:
        return False

    # Extract key arguments from each contribution
    args = [extract_key_arguments(c.content) for c in contributions]

    # Check for repetition
    seen_args = set()
    repeat_count = 0

    for arg_list in args:
        for arg in arg_list:
            if arg in seen_args:
                repeat_count += 1
            seen_args.add(arg)

    # If >60% of arguments are repeats = circular
    total_args = sum(len(a) for a in args)
    return (repeat_count / total_args) > 0.6 if total_args > 0 else False
```

### Integration with Facilitator Decision

```python
# In decide_next_action method

async def decide_next_action(self, state, round_number, max_rounds):
    """Decide next action with adaptive moderator checking."""

    # Check if moderator should intervene BEFORE calling LLM
    moderator_trigger = self._should_trigger_moderator(state, round_number)

    if moderator_trigger:
        logger.info(f"ğŸ­ Auto-triggering {moderator_trigger['type']} moderator")
        logger.info(f"   Reason: {moderator_trigger['reason']}")

        return FacilitatorDecision(
            action="moderator",
            reasoning=moderator_trigger['reason'],
            moderator_type=moderator_trigger['type'],
            moderator_focus=moderator_trigger['reason'],
        ), None  # Skip LLM call

    # Otherwise, call LLM for decision
    response = await self.broker.call(request)
    # ... rest of logic
```

---

## Solution 5: Research Triggers During Multi-Round

### Current State
Research option exists but not implemented

### Proposed Implementation

```python
# In bo1/agents/facilitator.py

def _check_research_needed(self, state: DeliberationState) -> dict | None:
    """Check if research/information is needed.

    Returns:
        {
            "query": str,
            "reason": str,
        } or None
    """
    recent = state.contributions[-3:]  # Last round

    # Look for questions or information gaps
    question_patterns = [
        "What is",
        "What are",
        "How much",
        "How many",
        "Do we know",
        "unclear",
        "uncertain",
        "need data",
        "need information",
    ]

    for contrib in recent:
        text = contrib.content
        for pattern in question_patterns:
            if pattern in text:
                # Extract the question/gap
                query = extract_research_query(text, pattern)

                return {
                    "query": query,
                    "reason": f"{contrib.persona_name} raised: {query}",
                }

    return None

# In decide_next_action
async def decide_next_action(self, state, round_number, max_rounds):
    """Decision with research checking."""

    # Check for research needs FIRST (fastest check)
    research_needed = self._check_research_needed(state)

    if research_needed:
        logger.info(f"ğŸ” Research needed: {research_needed['query']}")

        return FacilitatorDecision(
            action="research",
            reasoning=research_needed['reason'],
            research_query=research_needed['query'],
        ), None  # Skip LLM call

    # Then check moderator needs
    # Then call LLM if neither triggered
```

---

## Implementation Priority

### Phase 1: Quick Wins (This Week)
1. âœ… **Persona caching** - Already done (`bo1/data/__init__.py` - added @lru_cache)
2. âœ… **Enhanced facilitator logging** - Already done (`bo1/agents/facilitator.py` - added debug logs)
3. ğŸ”¨ **Haiku for facilitator** - 30 min implementation
   - File: `bo1/agents/facilitator.py`
   - Changes: Add `use_haiku` parameter to `__init__`, set model in request
   - Lines: ~32-40, ~99-106
4. ğŸ”¨ **Expose metrics in logs** - 1 hour implementation
   - File: `bo1/orchestration/deliberation.py`
   - Changes: Add `_calculate_round_metrics()` and helper methods
   - Lines: Add after run_round() method (~500-600)

### Phase 2: Adaptive System (Next Week)
5. ğŸ”¨ **Adaptive round prompts** - 2 hours implementation
   - **File 1**: `bo1/prompts/reusable_prompts.py`
     - Add `get_round_phase_config()` function (after line 591)
     - Update imports to include `Any` type
   - **File 2**: `bo1/orchestration/deliberation.py`
     - Update `_call_persona_async()` signature to include `round_number` parameter
     - Import `get_round_phase_config` from reusable_prompts
     - Apply adaptive config to user message and PromptRequest
     - Update all call sites: `run_initial_round()`, `run_round()`
     - Lines: ~10-15 (imports), ~250 (run_initial_round), ~450 (run_round), ~600-700 (_call_persona_async)
6. ğŸ”¨ **Adaptive moderator triggers** - 3 hours implementation
   - File: `bo1/agents/facilitator.py`
   - Changes: Add `_should_trigger_moderator()` and detection methods
   - Lines: Add before `decide_next_action()` method (~240-350)
7. ğŸ”¨ **Research triggers** - 2 hours implementation
   - File: `bo1/agents/facilitator.py`
   - Changes: Add `_check_research_needed()` method
   - Lines: Add after moderator detection methods (~390-430)

### Phase 3: Testing & Tuning (Following Week)
8. Test on 20 deliberations
9. Tune thresholds (convergence, novelty, conflict)
10. Measure improvements

---

## Expected Results

### Current State (Round 2-7)
```
Round 2: Facilitator (25s, $0.027) â†’ Persona speaks (57s, $0.040) = 82s total
Round 3: Facilitator (25s, $0.027) â†’ Persona speaks (52s, $0.038) = 77s total
Round 4: Facilitator (25s, $0.027) â†’ Persona speaks (55s, $0.041) = 80s total
Round 5: Facilitator (25s, $0.027) â†’ Persona speaks (48s, $0.035) = 73s total

Total for 4 rounds: ~312s (5.2 min), ~$0.288
```

### After Optimizations
```
Round 2 (early): Facilitator (4s, $0.002) â†’ Persona speaks (40s, $0.032) = 44s
Round 3 (early): Facilitator (4s, $0.002) â†’ Persona speaks (38s, $0.030) = 42s
Round 4 (middle): Facilitator (4s, $0.002) â†’ Persona speaks (32s, $0.024) = 36s
Round 5 (late): Facilitator (4s, $0.002) â†’ Persona speaks (22s, $0.016) = 26s

Total for 4 rounds: ~148s (2.5 min), ~$0.120
```

### Improvements
- âš¡ **Speed**: 5.2 min â†’ 2.5 min (53% faster)
- ğŸ’° **Cost**: $0.288 â†’ $0.120 (58% cheaper)
- ğŸ¯ **Quality**: Better convergence, less drift
- ğŸ“Š **Visibility**: Metrics exposed in logs

---

## Monitoring & Success Criteria

### Metrics to Track
- Average time per round (target: <40s)
- Cost per deliberation (target: <$0.15)
- Early stop rate (target: >30% of deliberations stop before max rounds)
- Consensus level at completion (target: >70%)
- User feedback on deliberation quality

### Dashboard
```
=== Deliberation Performance ===
Total rounds: 5 / 7
Total time: 2m 28s
Total cost: $0.118

Convergence trajectory:
Round 1: 0.32 â”â”â”â”â–‘â–‘â–‘â–‘â–‘â–‘
Round 2: 0.51 â”â”â”â”â”â–‘â–‘â–‘â–‘â–‘
Round 3: 0.68 â”â”â”â”â”â”â”â–‘â–‘â–‘
Round 4: 0.84 â”â”â”â”â”â”â”â”â–‘â–‘
Round 5: 0.91 â”â”â”â”â”â”â”â”â”â–‘ âœ“ Consensus reached

Moderator interventions: 1 (contrarian at round 3)
Research calls: 0
Early stop: YES (2 rounds saved)
```

---

## Notes

1. **All changes backward compatible**: Can enable/disable optimizations via config
2. **A/B testing ready**: Can compare old vs new approach on same problems
3. **Research-aligned**: All optimizations match CONSENSUS_BUILDING_RESEARCH.md recommendations
4. **User control**: Exposed metrics allow user to see what's happening and tune thresholds

